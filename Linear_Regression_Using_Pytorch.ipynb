{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1bDdEUJVc_Bo47xKJeA9wzPnKqUuvw47N",
      "authorship_tag": "ABX9TyOma4UqMYM0azFDyY4THN73"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARoRsQJDTBuR",
        "outputId": "2897987e-3cfc-4e8e-c870-424a2a97d01d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      price  bedrooms  bathrooms  sqft_living  sqft_lot  floors\n",
            "0    313000         3       1.50         1340      7912     1.5\n",
            "1   2384000         5       2.50         3650      9050     2.0\n",
            "2    342000         3       2.00         1930     11947     1.0\n",
            "3    420000         3       2.25         2000      8030     1.0\n",
            "4    550000         4       2.50         1940     10500     1.0\n",
            "..      ...       ...        ...          ...       ...     ...\n",
            "94   524000         3       1.75         1560      5520     1.0\n",
            "95   541125         5       2.75         2740      8426     1.0\n",
            "96   670000         3       2.50         1680      2000     3.0\n",
            "97   384900         5       2.50         3090     12750     1.0\n",
            "98   406100         3       2.25         1410      1332     3.0\n",
            "\n",
            "[99 rows x 6 columns]\n"
          ]
        }
      ],
      "source": [
        "import torch as pt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "path = \"/content/drive/MyDrive/Internship/\"\n",
        "df = pd.read_csv(path + \"data_mod.csv\")\n",
        "print(df)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = pt.tensor(df.values)\n",
        "x = tensor[:,1:]\n",
        "y = tensor[:,0]\n",
        "y = y.reshape(-1,1)\n",
        "\n",
        "print(f\"Values of features as tensor before nomalization:\\n{x}\")\n",
        "print(x.shape)\n",
        "print(f\"\\nValue of output as tensor before normlaization:\\n{y}\")\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pd7GGGPbKjQH",
        "outputId": "533f38f2-dd8a-4b35-dbfd-d2d22777668e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Values of features as tensor:\n",
            "tensor([[3.0000e+00, 1.5000e+00, 1.3400e+03, 7.9120e+03, 1.5000e+00],\n",
            "        [5.0000e+00, 2.5000e+00, 3.6500e+03, 9.0500e+03, 2.0000e+00],\n",
            "        [3.0000e+00, 2.0000e+00, 1.9300e+03, 1.1947e+04, 1.0000e+00],\n",
            "        [3.0000e+00, 2.2500e+00, 2.0000e+03, 8.0300e+03, 1.0000e+00],\n",
            "        [4.0000e+00, 2.5000e+00, 1.9400e+03, 1.0500e+04, 1.0000e+00],\n",
            "        [2.0000e+00, 1.0000e+00, 8.8000e+02, 6.3800e+03, 1.0000e+00],\n",
            "        [2.0000e+00, 2.0000e+00, 1.3500e+03, 2.5600e+03, 1.0000e+00],\n",
            "        [4.0000e+00, 2.5000e+00, 2.7100e+03, 3.5868e+04, 2.0000e+00],\n",
            "        [3.0000e+00, 2.5000e+00, 2.4300e+03, 8.8426e+04, 1.0000e+00],\n",
            "        [4.0000e+00, 2.0000e+00, 1.5200e+03, 6.2000e+03, 1.5000e+00],\n",
            "        [3.0000e+00, 1.7500e+00, 1.7100e+03, 7.3200e+03, 1.0000e+00],\n",
            "        [4.0000e+00, 2.5000e+00, 2.9200e+03, 4.0000e+03, 1.5000e+00],\n",
            "        [3.0000e+00, 1.7500e+00, 2.3300e+03, 1.4892e+04, 1.0000e+00],\n",
            "        [3.0000e+00, 1.0000e+00, 1.0900e+03, 6.4350e+03, 1.0000e+00],\n",
            "        [5.0000e+00, 2.7500e+00, 2.9100e+03, 9.4800e+03, 1.5000e+00],\n",
            "        [3.0000e+00, 1.5000e+00, 1.2000e+03, 9.7200e+03, 1.0000e+00],\n",
            "        [3.0000e+00, 1.5000e+00, 1.5700e+03, 6.7000e+03, 1.0000e+00],\n",
            "        [4.0000e+00, 3.0000e+00, 3.1100e+03, 7.2310e+03, 2.0000e+00],\n",
            "        [3.0000e+00, 1.7500e+00, 1.3700e+03, 5.8580e+03, 1.0000e+00],\n",
            "        [3.0000e+00, 1.5000e+00, 1.1800e+03, 1.0277e+04, 1.0000e+00],\n",
            "        [3.0000e+00, 1.7500e+00, 2.2400e+03, 1.0578e+04, 2.0000e+00],\n",
            "        [4.0000e+00, 1.0000e+00, 1.4500e+03, 8.8000e+03, 1.0000e+00],\n",
            "        [3.0000e+00, 2.2500e+00, 1.7500e+03, 1.5720e+03, 2.5000e+00],\n",
            "        [4.0000e+00, 2.5000e+00, 2.7300e+03, 1.2261e+04, 2.0000e+00],\n",
            "        [4.0000e+00, 1.7500e+00, 1.6000e+03, 6.3800e+03, 1.0000e+00],\n",
            "        [3.0000e+00, 2.5000e+00, 2.0900e+03, 1.0834e+04, 1.0000e+00],\n",
            "        [3.0000e+00, 1.7500e+00, 2.3600e+03, 7.2910e+03, 1.0000e+00],\n",
            "        [4.0000e+00, 2.2500e+00, 2.2000e+03, 1.1250e+04, 1.5000e+00],\n",
            "        [5.0000e+00, 2.5000e+00, 2.8200e+03, 6.7518e+04, 2.0000e+00],\n",
            "        [3.0000e+00, 2.5000e+00, 2.6000e+03, 4.7500e+03, 1.0000e+00],\n",
            "        [4.0000e+00, 1.7500e+00, 1.5600e+03, 8.7000e+03, 1.0000e+00],\n",
            "        [4.0000e+00, 2.5000e+00, 2.8600e+03, 3.3450e+03, 2.0000e+00],\n",
            "        [4.0000e+00, 2.0000e+00, 1.8200e+03, 5.0000e+03, 1.5000e+00],\n",
            "        [4.0000e+00, 2.5000e+00, 2.8200e+03, 8.4080e+03, 2.0000e+00],\n",
            "        [4.0000e+00, 2.5000e+00, 3.6300e+03, 4.2884e+04, 1.5000e+00],\n",
            "        [3.0000e+00, 2.5000e+00, 3.2400e+03, 3.3151e+04, 2.0000e+00],\n",
            "        [2.0000e+00, 1.0000e+00, 8.0000e+02, 4.8500e+03, 1.0000e+00],\n",
            "        [3.0000e+00, 3.0000e+00, 1.8500e+03, 1.9966e+04, 1.0000e+00],\n",
            "        [3.0000e+00, 2.0000e+00, 1.9600e+03, 1.3100e+04, 1.0000e+00],\n",
            "        [3.0000e+00, 2.5000e+00, 2.3900e+03, 6.5500e+03, 1.0000e+00],\n",
            "        [3.0000e+00, 2.2500e+00, 1.5800e+03, 1.6215e+04, 1.0000e+00],\n",
            "        [4.0000e+00, 2.0000e+00, 1.4800e+03, 8.6250e+03, 1.0000e+00],\n",
            "        [2.0000e+00, 1.0000e+00, 8.5000e+02, 6.1740e+03, 1.0000e+00],\n",
            "        [3.0000e+00, 2.5000e+00, 1.7700e+03, 2.8750e+03, 2.0000e+00],\n",
            "        [2.0000e+00, 1.0000e+00, 1.2100e+03, 9.4000e+03, 1.0000e+00],\n",
            "        [3.0000e+00, 1.0000e+00, 1.1600e+03, 9.1800e+03, 1.0000e+00],\n",
            "        [3.0000e+00, 2.2500e+00, 1.9700e+03, 3.5100e+04, 2.0000e+00],\n",
            "        [3.0000e+00, 2.5000e+00, 1.9300e+03, 1.0460e+04, 2.0000e+00],\n",
            "        [3.0000e+00, 2.5000e+00, 1.2700e+03, 1.1800e+03, 3.0000e+00],\n",
            "        [4.0000e+00, 2.5000e+00, 3.3100e+03, 4.2998e+04, 2.0000e+00],\n",
            "        [4.0000e+00, 2.7500e+00, 2.7100e+03, 3.7277e+04, 2.0000e+00],\n",
            "        [3.0000e+00, 1.7500e+00, 2.9100e+03, 3.5200e+04, 1.5000e+00],\n",
            "        [3.0000e+00, 2.0000e+00, 2.7100e+03, 4.5000e+03, 1.5000e+00],\n",
            "        [3.0000e+00, 1.7500e+00, 1.8000e+03, 2.3103e+04, 1.0000e+00],\n",
            "        [5.0000e+00, 2.5000e+00, 2.2100e+03, 9.6550e+03, 1.0000e+00],\n",
            "        [4.0000e+00, 1.5000e+00, 1.9200e+03, 1.0000e+04, 1.0000e+00],\n",
            "        [4.0000e+00, 1.7500e+00, 2.1900e+03, 1.2545e+05, 1.0000e+00],\n",
            "        [5.0000e+00, 3.2500e+00, 3.6600e+03, 1.1995e+04, 2.0000e+00],\n",
            "        [3.0000e+00, 3.2500e+00, 1.3200e+03, 1.3270e+03, 2.0000e+00],\n",
            "        [2.0000e+00, 2.5000e+00, 1.6300e+03, 1.3680e+03, 2.0000e+00],\n",
            "        [4.0000e+00, 2.5000e+00, 3.3100e+03, 6.5000e+03, 2.0000e+00],\n",
            "        [3.0000e+00, 2.5000e+00, 2.6800e+03, 5.5390e+03, 2.0000e+00],\n",
            "        [4.0000e+00, 3.2500e+00, 2.7300e+03, 5.4014e+04, 1.0000e+00],\n",
            "        [3.0000e+00, 2.5000e+00, 2.5400e+03, 5.0500e+03, 2.0000e+00],\n",
            "        [4.0000e+00, 2.7500e+00, 2.9200e+03, 6.6050e+03, 2.0000e+00],\n",
            "        [2.0000e+00, 1.0000e+00, 9.0000e+02, 5.0000e+03, 1.0000e+00],\n",
            "        [3.0000e+00, 2.5000e+00, 1.6500e+03, 1.4054e+04, 1.0000e+00],\n",
            "        [3.0000e+00, 1.0000e+00, 1.0100e+03, 9.9450e+03, 1.0000e+00],\n",
            "        [2.0000e+00, 2.5000e+00, 2.6800e+03, 7.3920e+03, 1.0000e+00],\n",
            "        [3.0000e+00, 2.5000e+00, 2.2000e+03, 7.3500e+03, 1.0000e+00],\n",
            "        [4.0000e+00, 2.5000e+00, 2.5300e+03, 9.9330e+03, 2.0000e+00],\n",
            "        [3.0000e+00, 2.5000e+00, 2.8500e+03, 7.1300e+03, 2.0000e+00],\n",
            "        [4.0000e+00, 2.0000e+00, 2.2800e+03, 6.0100e+03, 1.0000e+00],\n",
            "        [3.0000e+00, 2.5000e+00, 1.9000e+03, 8.7440e+03, 2.0000e+00],\n",
            "        [3.0000e+00, 1.0000e+00, 1.3300e+03, 7.7972e+04, 1.0000e+00],\n",
            "        [2.0000e+00, 3.0000e+00, 1.2700e+03, 1.1750e+03, 2.0000e+00],\n",
            "        [4.0000e+00, 3.0000e+00, 2.1700e+03, 4.0000e+03, 2.0000e+00],\n",
            "        [4.0000e+00, 2.5000e+00, 2.7300e+03, 1.0281e+04, 2.0000e+00],\n",
            "        [4.0000e+00, 2.5000e+00, 2.1200e+03, 1.0202e+04, 1.0000e+00],\n",
            "        [3.0000e+00, 2.5000e+00, 2.4900e+03, 4.3430e+03, 2.0000e+00],\n",
            "        [4.0000e+00, 2.2500e+00, 1.9900e+03, 7.7120e+03, 1.0000e+00],\n",
            "        [4.0000e+00, 3.0000e+00, 3.6900e+03, 9.8920e+03, 2.0000e+00],\n",
            "        [2.0000e+00, 1.0000e+00, 2.5400e+03, 7.0000e+03, 1.0000e+00],\n",
            "        [3.0000e+00, 2.2500e+00, 1.8000e+03, 3.6704e+04, 1.0000e+00],\n",
            "        [4.0000e+00, 3.0000e+00, 2.3400e+03, 7.0480e+03, 1.0000e+00],\n",
            "        [2.0000e+00, 1.7500e+00, 2.1100e+03, 9.5190e+03, 1.0000e+00],\n",
            "        [2.0000e+00, 1.0000e+00, 1.1400e+03, 5.4000e+03, 1.0000e+00],\n",
            "        [3.0000e+00, 1.7500e+00, 1.4800e+03, 2.1850e+03, 2.5000e+00],\n",
            "        [2.0000e+00, 3.0000e+00, 1.4100e+03, 1.2400e+03, 2.0000e+00],\n",
            "        [2.0000e+00, 1.0000e+00, 1.2000e+03, 8.0630e+03, 1.0000e+00],\n",
            "        [5.0000e+00, 2.5000e+00, 3.1800e+03, 1.3806e+04, 2.0000e+00],\n",
            "        [3.0000e+00, 1.5000e+00, 2.1600e+03, 9.0000e+03, 1.0000e+00],\n",
            "        [6.0000e+00, 1.7500e+00, 2.9200e+03, 5.0000e+03, 2.5000e+00],\n",
            "        [3.0000e+00, 2.0000e+00, 2.3500e+03, 5.7000e+03, 1.5000e+00],\n",
            "        [3.0000e+00, 1.7500e+00, 1.5600e+03, 5.5200e+03, 1.0000e+00],\n",
            "        [5.0000e+00, 2.7500e+00, 2.7400e+03, 8.4260e+03, 1.0000e+00],\n",
            "        [3.0000e+00, 2.5000e+00, 1.6800e+03, 2.0000e+03, 3.0000e+00],\n",
            "        [5.0000e+00, 2.5000e+00, 3.0900e+03, 1.2750e+04, 1.0000e+00],\n",
            "        [3.0000e+00, 2.2500e+00, 1.4100e+03, 1.3320e+03, 3.0000e+00]],\n",
            "       dtype=torch.float64)\n",
            "torch.Size([99, 5])\n",
            "\n",
            "Value of output as tensor:\n",
            "tensor([[ 313000.],\n",
            "        [2384000.],\n",
            "        [ 342000.],\n",
            "        [ 420000.],\n",
            "        [ 550000.],\n",
            "        [ 490000.],\n",
            "        [ 335000.],\n",
            "        [ 482000.],\n",
            "        [ 452500.],\n",
            "        [ 640000.],\n",
            "        [ 463000.],\n",
            "        [1400000.],\n",
            "        [ 588500.],\n",
            "        [ 365000.],\n",
            "        [1200000.],\n",
            "        [ 242500.],\n",
            "        [ 419000.],\n",
            "        [ 367500.],\n",
            "        [ 257950.],\n",
            "        [ 275000.],\n",
            "        [ 750000.],\n",
            "        [ 435000.],\n",
            "        [ 626000.],\n",
            "        [ 612500.],\n",
            "        [ 495000.],\n",
            "        [ 285000.],\n",
            "        [ 615000.],\n",
            "        [ 698000.],\n",
            "        [ 675000.],\n",
            "        [ 790000.],\n",
            "        [ 382500.],\n",
            "        [ 499950.],\n",
            "        [ 650000.],\n",
            "        [ 625000.],\n",
            "        [ 400000.],\n",
            "        [ 604000.],\n",
            "        [ 440000.],\n",
            "        [ 287200.],\n",
            "        [ 403000.],\n",
            "        [ 750000.],\n",
            "        [ 335000.],\n",
            "        [ 260000.],\n",
            "        [ 308500.],\n",
            "        [ 439950.],\n",
            "        [ 235000.],\n",
            "        [ 315000.],\n",
            "        [ 437500.],\n",
            "        [ 407500.],\n",
            "        [ 445700.],\n",
            "        [ 838000.],\n",
            "        [ 630000.],\n",
            "        [ 550000.],\n",
            "        [ 805000.],\n",
            "        [ 284000.],\n",
            "        [ 470000.],\n",
            "        [ 430000.],\n",
            "        [ 491500.],\n",
            "        [ 785000.],\n",
            "        [ 385000.],\n",
            "        [ 295000.],\n",
            "        [ 555000.],\n",
            "        [ 459990.],\n",
            "        [ 625000.],\n",
            "        [ 300000.],\n",
            "        [ 625000.],\n",
            "        [ 553000.],\n",
            "        [ 379880.],\n",
            "        [ 310000.],\n",
            "        [ 775000.],\n",
            "        [ 365000.],\n",
            "        [ 331950.],\n",
            "        [ 783500.],\n",
            "        [ 628000.],\n",
            "        [ 560000.],\n",
            "        [ 900000.],\n",
            "        [ 531000.],\n",
            "        [ 831000.],\n",
            "        [ 780000.],\n",
            "        [ 755000.],\n",
            "        [ 705380.],\n",
            "        [ 627000.],\n",
            "        [ 865000.],\n",
            "        [ 580000.],\n",
            "        [ 410000.],\n",
            "        [ 485000.],\n",
            "        [ 400000.],\n",
            "        [ 549000.],\n",
            "        [ 620000.],\n",
            "        [ 405000.],\n",
            "        [ 232000.],\n",
            "        [ 800866.],\n",
            "        [ 650000.],\n",
            "        [ 823000.],\n",
            "        [ 770000.],\n",
            "        [ 524000.],\n",
            "        [ 541125.],\n",
            "        [ 670000.],\n",
            "        [ 384900.],\n",
            "        [ 406100.]], dtype=torch.float64)\n",
            "torch.Size([99, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scalerX = MinMaxScaler()\n",
        "x_norm = scalerX.fit_transform(x)\n",
        "scalerY = MinMaxScaler()\n",
        "y_norm = scalerY.fit_transform(y)\n",
        "x = pt.from_numpy(x_norm)\n",
        "y = pt.from_numpy(y_norm)\n",
        "\n",
        "print(f\"Values of features as tensor after nomalization:\\n{x}\")\n",
        "print(x.shape)\n",
        "print(f\"\\nValue of output as tensor after normlaization:\\n{y}\")\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dnTIjPCP8oi",
        "outputId": "9e000b37-33ae-433f-b6a4-6e1650a7efd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Values of features as tensor after nomalization:\n",
            "tensor([[2.5000e-01, 2.2222e-01, 1.8685e-01, 5.4210e-02, 2.5000e-01],\n",
            "        [7.5000e-01, 6.6667e-01, 9.8616e-01, 6.3367e-02, 5.0000e-01],\n",
            "        [2.5000e-01, 4.4444e-01, 3.9100e-01, 8.6677e-02, 0.0000e+00],\n",
            "        [2.5000e-01, 5.5556e-01, 4.1522e-01, 5.5159e-02, 0.0000e+00],\n",
            "        [5.0000e-01, 6.6667e-01, 3.9446e-01, 7.5034e-02, 0.0000e+00],\n",
            "        [0.0000e+00, 0.0000e+00, 2.7682e-02, 4.1882e-02, 0.0000e+00],\n",
            "        [0.0000e+00, 4.4444e-01, 1.9031e-01, 1.1144e-02, 0.0000e+00],\n",
            "        [5.0000e-01, 6.6667e-01, 6.6090e-01, 2.7916e-01, 5.0000e-01],\n",
            "        [2.5000e-01, 6.6667e-01, 5.6401e-01, 7.0207e-01, 0.0000e+00],\n",
            "        [5.0000e-01, 4.4444e-01, 2.4913e-01, 4.0434e-02, 2.5000e-01],\n",
            "        [2.5000e-01, 3.3333e-01, 3.1488e-01, 4.9446e-02, 0.0000e+00],\n",
            "        [5.0000e-01, 6.6667e-01, 7.3356e-01, 2.2731e-02, 2.5000e-01],\n",
            "        [2.5000e-01, 3.3333e-01, 5.2941e-01, 1.1037e-01, 0.0000e+00],\n",
            "        [2.5000e-01, 0.0000e+00, 1.0035e-01, 4.2325e-02, 0.0000e+00],\n",
            "        [7.5000e-01, 7.7778e-01, 7.3010e-01, 6.6827e-02, 2.5000e-01],\n",
            "        [2.5000e-01, 2.2222e-01, 1.3841e-01, 6.8758e-02, 0.0000e+00],\n",
            "        [2.5000e-01, 2.2222e-01, 2.6644e-01, 4.4457e-02, 0.0000e+00],\n",
            "        [5.0000e-01, 8.8889e-01, 7.9931e-01, 4.8730e-02, 5.0000e-01],\n",
            "        [2.5000e-01, 3.3333e-01, 1.9723e-01, 3.7682e-02, 0.0000e+00],\n",
            "        [2.5000e-01, 2.2222e-01, 1.3149e-01, 7.3240e-02, 0.0000e+00],\n",
            "        [2.5000e-01, 3.3333e-01, 4.9827e-01, 7.5662e-02, 5.0000e-01],\n",
            "        [5.0000e-01, 0.0000e+00, 2.2491e-01, 6.1355e-02, 0.0000e+00],\n",
            "        [2.5000e-01, 5.5556e-01, 3.2872e-01, 3.1945e-03, 7.5000e-01],\n",
            "        [5.0000e-01, 6.6667e-01, 6.6782e-01, 8.9204e-02, 5.0000e-01],\n",
            "        [5.0000e-01, 3.3333e-01, 2.7682e-01, 4.1882e-02, 0.0000e+00],\n",
            "        [2.5000e-01, 6.6667e-01, 4.4637e-01, 7.7722e-02, 0.0000e+00],\n",
            "        [2.5000e-01, 3.3333e-01, 5.3979e-01, 4.9213e-02, 0.0000e+00],\n",
            "        [5.0000e-01, 5.5556e-01, 4.8443e-01, 8.1069e-02, 2.5000e-01],\n",
            "        [7.5000e-01, 6.6667e-01, 6.9896e-01, 5.3383e-01, 5.0000e-01],\n",
            "        [2.5000e-01, 6.6667e-01, 6.2284e-01, 2.8766e-02, 0.0000e+00],\n",
            "        [5.0000e-01, 3.3333e-01, 2.6298e-01, 6.0550e-02, 0.0000e+00],\n",
            "        [5.0000e-01, 6.6667e-01, 7.1280e-01, 1.7461e-02, 5.0000e-01],\n",
            "        [5.0000e-01, 4.4444e-01, 3.5294e-01, 3.0778e-02, 2.5000e-01],\n",
            "        [5.0000e-01, 6.6667e-01, 6.9896e-01, 5.8201e-02, 5.0000e-01],\n",
            "        [5.0000e-01, 6.6667e-01, 9.7924e-01, 3.3561e-01, 2.5000e-01],\n",
            "        [2.5000e-01, 6.6667e-01, 8.4429e-01, 2.5730e-01, 5.0000e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 2.9571e-02, 0.0000e+00],\n",
            "        [2.5000e-01, 8.8889e-01, 3.6332e-01, 1.5120e-01, 0.0000e+00],\n",
            "        [2.5000e-01, 4.4444e-01, 4.0138e-01, 9.5955e-02, 0.0000e+00],\n",
            "        [2.5000e-01, 6.6667e-01, 5.5017e-01, 4.3250e-02, 0.0000e+00],\n",
            "        [2.5000e-01, 5.5556e-01, 2.6990e-01, 1.2102e-01, 0.0000e+00],\n",
            "        [5.0000e-01, 4.4444e-01, 2.3529e-01, 5.9947e-02, 0.0000e+00],\n",
            "        [0.0000e+00, 0.0000e+00, 1.7301e-02, 4.0225e-02, 0.0000e+00],\n",
            "        [2.5000e-01, 6.6667e-01, 3.3564e-01, 1.3679e-02, 5.0000e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 1.4187e-01, 6.6183e-02, 0.0000e+00],\n",
            "        [2.5000e-01, 0.0000e+00, 1.2457e-01, 6.4413e-02, 0.0000e+00],\n",
            "        [2.5000e-01, 5.5556e-01, 4.0484e-01, 2.7298e-01, 5.0000e-01],\n",
            "        [2.5000e-01, 6.6667e-01, 3.9100e-01, 7.4712e-02, 5.0000e-01],\n",
            "        [2.5000e-01, 6.6667e-01, 1.6263e-01, 4.0233e-05, 1.0000e+00],\n",
            "        [5.0000e-01, 6.6667e-01, 8.6851e-01, 3.3653e-01, 5.0000e-01],\n",
            "        [5.0000e-01, 7.7778e-01, 6.6090e-01, 2.9050e-01, 5.0000e-01],\n",
            "        [2.5000e-01, 3.3333e-01, 7.3010e-01, 2.7378e-01, 2.5000e-01],\n",
            "        [2.5000e-01, 4.4444e-01, 6.6090e-01, 2.6755e-02, 2.5000e-01],\n",
            "        [2.5000e-01, 3.3333e-01, 3.4602e-01, 1.7644e-01, 0.0000e+00],\n",
            "        [7.5000e-01, 6.6667e-01, 4.8789e-01, 6.8235e-02, 0.0000e+00],\n",
            "        [5.0000e-01, 2.2222e-01, 3.8754e-01, 7.1011e-02, 0.0000e+00],\n",
            "        [5.0000e-01, 3.3333e-01, 4.8097e-01, 1.0000e+00, 0.0000e+00],\n",
            "        [7.5000e-01, 1.0000e+00, 9.8962e-01, 8.7064e-02, 5.0000e-01],\n",
            "        [2.5000e-01, 1.0000e+00, 1.7993e-01, 1.2231e-03, 5.0000e-01],\n",
            "        [0.0000e+00, 6.6667e-01, 2.8720e-01, 1.5530e-03, 5.0000e-01],\n",
            "        [5.0000e-01, 6.6667e-01, 8.6851e-01, 4.2848e-02, 5.0000e-01],\n",
            "        [2.5000e-01, 6.6667e-01, 6.5052e-01, 3.5115e-02, 5.0000e-01],\n",
            "        [5.0000e-01, 1.0000e+00, 6.6782e-01, 4.2517e-01, 0.0000e+00],\n",
            "        [2.5000e-01, 6.6667e-01, 6.0208e-01, 3.1180e-02, 5.0000e-01],\n",
            "        [5.0000e-01, 7.7778e-01, 7.3356e-01, 4.3693e-02, 5.0000e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 3.4602e-02, 3.0778e-02, 0.0000e+00],\n",
            "        [2.5000e-01, 6.6667e-01, 2.9412e-01, 1.0363e-01, 0.0000e+00],\n",
            "        [2.5000e-01, 0.0000e+00, 7.2664e-02, 7.0568e-02, 0.0000e+00],\n",
            "        [0.0000e+00, 6.6667e-01, 6.5052e-01, 5.0025e-02, 0.0000e+00],\n",
            "        [2.5000e-01, 6.6667e-01, 4.8443e-01, 4.9687e-02, 0.0000e+00],\n",
            "        [5.0000e-01, 6.6667e-01, 5.9862e-01, 7.0472e-02, 5.0000e-01],\n",
            "        [2.5000e-01, 6.6667e-01, 7.0934e-01, 4.7917e-02, 5.0000e-01],\n",
            "        [5.0000e-01, 4.4444e-01, 5.1211e-01, 3.8905e-02, 0.0000e+00],\n",
            "        [2.5000e-01, 6.6667e-01, 3.8062e-01, 6.0904e-02, 5.0000e-01],\n",
            "        [2.5000e-01, 0.0000e+00, 1.8339e-01, 6.1795e-01, 0.0000e+00],\n",
            "        [0.0000e+00, 8.8889e-01, 1.6263e-01, 0.0000e+00, 5.0000e-01],\n",
            "        [5.0000e-01, 8.8889e-01, 4.7405e-01, 2.2731e-02, 5.0000e-01],\n",
            "        [5.0000e-01, 6.6667e-01, 6.6782e-01, 7.3272e-02, 5.0000e-01],\n",
            "        [5.0000e-01, 6.6667e-01, 4.5675e-01, 7.2636e-02, 0.0000e+00],\n",
            "        [2.5000e-01, 6.6667e-01, 5.8478e-01, 2.5491e-02, 5.0000e-01],\n",
            "        [5.0000e-01, 5.5556e-01, 4.1176e-01, 5.2600e-02, 0.0000e+00],\n",
            "        [5.0000e-01, 8.8889e-01, 1.0000e+00, 7.0142e-02, 5.0000e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 6.0208e-01, 4.6871e-02, 0.0000e+00],\n",
            "        [2.5000e-01, 5.5556e-01, 3.4602e-01, 2.8589e-01, 0.0000e+00],\n",
            "        [5.0000e-01, 8.8889e-01, 5.3287e-01, 4.7257e-02, 0.0000e+00],\n",
            "        [0.0000e+00, 3.3333e-01, 4.5329e-01, 6.7140e-02, 0.0000e+00],\n",
            "        [0.0000e+00, 0.0000e+00, 1.1765e-01, 3.3997e-02, 0.0000e+00],\n",
            "        [2.5000e-01, 3.3333e-01, 2.3529e-01, 8.1270e-03, 7.5000e-01],\n",
            "        [0.0000e+00, 8.8889e-01, 2.1107e-01, 5.2303e-04, 5.0000e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 1.3841e-01, 5.5425e-02, 0.0000e+00],\n",
            "        [7.5000e-01, 6.6667e-01, 8.2353e-01, 1.0164e-01, 5.0000e-01],\n",
            "        [2.5000e-01, 2.2222e-01, 4.7059e-01, 6.2964e-02, 0.0000e+00],\n",
            "        [1.0000e+00, 3.3333e-01, 7.3356e-01, 3.0778e-02, 7.5000e-01],\n",
            "        [2.5000e-01, 4.4444e-01, 5.3633e-01, 3.6411e-02, 2.5000e-01],\n",
            "        [2.5000e-01, 3.3333e-01, 2.6298e-01, 3.4962e-02, 0.0000e+00],\n",
            "        [7.5000e-01, 7.7778e-01, 6.7128e-01, 5.8345e-02, 0.0000e+00],\n",
            "        [2.5000e-01, 6.6667e-01, 3.0450e-01, 6.6384e-03, 1.0000e+00],\n",
            "        [7.5000e-01, 6.6667e-01, 7.9239e-01, 9.3139e-02, 0.0000e+00],\n",
            "        [2.5000e-01, 5.5556e-01, 2.1107e-01, 1.2633e-03, 1.0000e+00]],\n",
            "       dtype=torch.float64)\n",
            "torch.Size([99, 5])\n",
            "\n",
            "Value of output as tensor after normlaization:\n",
            "tensor([[0.0376],\n",
            "        [1.0000],\n",
            "        [0.0511],\n",
            "        [0.0874],\n",
            "        [0.1478],\n",
            "        [0.1199],\n",
            "        [0.0479],\n",
            "        [0.1162],\n",
            "        [0.1025],\n",
            "        [0.1896],\n",
            "        [0.1073],\n",
            "        [0.5428],\n",
            "        [0.1657],\n",
            "        [0.0618],\n",
            "        [0.4498],\n",
            "        [0.0049],\n",
            "        [0.0869],\n",
            "        [0.0630],\n",
            "        [0.0121],\n",
            "        [0.0200],\n",
            "        [0.2407],\n",
            "        [0.0943],\n",
            "        [0.1831],\n",
            "        [0.1768],\n",
            "        [0.1222],\n",
            "        [0.0246],\n",
            "        [0.1780],\n",
            "        [0.2165],\n",
            "        [0.2059],\n",
            "        [0.2593],\n",
            "        [0.0699],\n",
            "        [0.1245],\n",
            "        [0.1942],\n",
            "        [0.1826],\n",
            "        [0.0781],\n",
            "        [0.1729],\n",
            "        [0.0967],\n",
            "        [0.0257],\n",
            "        [0.0795],\n",
            "        [0.2407],\n",
            "        [0.0479],\n",
            "        [0.0130],\n",
            "        [0.0355],\n",
            "        [0.0966],\n",
            "        [0.0014],\n",
            "        [0.0386],\n",
            "        [0.0955],\n",
            "        [0.0816],\n",
            "        [0.0993],\n",
            "        [0.2816],\n",
            "        [0.1849],\n",
            "        [0.1478],\n",
            "        [0.2663],\n",
            "        [0.0242],\n",
            "        [0.1106],\n",
            "        [0.0920],\n",
            "        [0.1206],\n",
            "        [0.2570],\n",
            "        [0.0711],\n",
            "        [0.0293],\n",
            "        [0.1501],\n",
            "        [0.1059],\n",
            "        [0.1826],\n",
            "        [0.0316],\n",
            "        [0.1826],\n",
            "        [0.1492],\n",
            "        [0.0687],\n",
            "        [0.0362],\n",
            "        [0.2523],\n",
            "        [0.0618],\n",
            "        [0.0464],\n",
            "        [0.2563],\n",
            "        [0.1840],\n",
            "        [0.1524],\n",
            "        [0.3104],\n",
            "        [0.1389],\n",
            "        [0.2783],\n",
            "        [0.2546],\n",
            "        [0.2430],\n",
            "        [0.2200],\n",
            "        [0.1836],\n",
            "        [0.2941],\n",
            "        [0.1617],\n",
            "        [0.0827],\n",
            "        [0.1176],\n",
            "        [0.0781],\n",
            "        [0.1473],\n",
            "        [0.1803],\n",
            "        [0.0804],\n",
            "        [0.0000],\n",
            "        [0.2643],\n",
            "        [0.1942],\n",
            "        [0.2746],\n",
            "        [0.2500],\n",
            "        [0.1357],\n",
            "        [0.1436],\n",
            "        [0.2035],\n",
            "        [0.0711],\n",
            "        [0.0809]], dtype=torch.float64)\n",
            "torch.Size([99, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def hypothesis(x,weights,bias):\n",
        "  return x @ weights + bias\n",
        "\n",
        "def cost_calculator(preds,y):\n",
        "    m = len(y)\n",
        "    errors = preds - y\n",
        "    cost = (1 / (2 * m)) * pt.sum(errors ** 2)\n",
        "    return cost\n",
        "\n",
        "def precent_calc(iCost, fCost ):\n",
        "  return ((iCost - fCost)/iCost)*100"
      ],
      "metadata": {
        "id": "ePkTFsuXLem0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pt.manual_seed(44)\n",
        "weights = pt.randn(5,1,requires_grad=True, dtype=pt.float64)\n",
        "bias = pt.randn(1,requires_grad=True,dtype=pt.float64 )\n",
        "preds = hypothesis(x,weights,bias)\n",
        "cost = cost_calculator(preds,y)\n",
        "initial_cost = cost\n",
        "\n",
        "\n",
        "print(f\"Initial cost: {cost}\")\n",
        "print(f\"\\nInitial weights: \\n{weights}\")\n",
        "print(f\"\\nInitial bias: \\n{bias}\")\n",
        "print(f\"\\nPredicted Output values: \\n{preds}\")\n",
        "print(preds.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBl7WabOLOlr",
        "outputId": "a3eeb6e2-e988-4499-c85c-b6409b341f4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial cost: 0.9773365967640137\n",
            "\n",
            "Initial weights: \n",
            "tensor([[ 0.0589],\n",
            "        [-1.3945],\n",
            "        [ 0.8448],\n",
            "        [ 0.1268],\n",
            "        [ 1.0822]], dtype=torch.float64, requires_grad=True)\n",
            "\n",
            "Initial bias: \n",
            "tensor([-1.1731], dtype=torch.float64, requires_grad=True)\n",
            "\n",
            "Predicted Output values: \n",
            "tensor([[-1.0329],\n",
            "        [-0.6763],\n",
            "        [-1.4368],\n",
            "        [-1.5753],\n",
            "        [-1.7305],\n",
            "        [-1.1444],\n",
            "        [-1.6307],\n",
            "        [-0.9384],\n",
            "        [-1.5225],\n",
            "        [-1.2772],\n",
            "        [-1.3509],\n",
            "        [-1.1801],\n",
            "        [-1.1619],\n",
            "        [-1.0682],\n",
            "        [-1.3176],\n",
            "        [-1.3426],\n",
            "        [-1.2375],\n",
            "        [-1.1606],\n",
            "        [-1.4518],\n",
            "        [-1.3479],\n",
            "        [-0.6515],\n",
            "        [-0.9458],\n",
            "        [-0.8433],\n",
            "        [-0.9567],\n",
            "        [-1.3693],\n",
            "        [-1.7011],\n",
            "        [-1.1609],\n",
            "        [-1.2282],\n",
            "        [-0.8592],\n",
            "        [-1.5582],\n",
            "        [-1.3786],\n",
            "        [-0.9278],\n",
            "        [-1.1908],\n",
            "        [-0.9343],\n",
            "        [-0.9329],\n",
            "        [-0.8010],\n",
            "        [-1.1693],\n",
            "        [-2.0718],\n",
            "        [-1.4269],\n",
            "        [-1.6177],\n",
            "        [-1.6897],\n",
            "        [-1.5570],\n",
            "        [-1.1534],\n",
            "        [-1.2616],\n",
            "        [-1.0448],\n",
            "        [-1.0450],\n",
            "        [-1.0153],\n",
            "        [-1.2071],\n",
            "        [-0.8684],\n",
            "        [-0.7558],\n",
            "        [-1.0919],\n",
            "        [-0.7011],\n",
            "        [-0.9458],\n",
            "        [-1.3085],\n",
            "        [-1.6377],\n",
            "        [-1.1171],\n",
            "        [-1.0753],\n",
            "        [-1.1352],\n",
            "        [-1.8595],\n",
            "        [-1.3188],\n",
            "        [-0.7930],\n",
            "        [-0.9929],\n",
            "        [-1.9200],\n",
            "        [-1.0343],\n",
            "        [-1.0618],\n",
            "        [-1.1400],\n",
            "        [-1.8264],\n",
            "        [-1.0880],\n",
            "        [-1.5468],\n",
            "        [-1.6725],\n",
            "        [-1.0175],\n",
            "        [-0.9416],\n",
            "        [-1.3258],\n",
            "        [-1.2176],\n",
            "        [-0.9251],\n",
            "        [-1.7341],\n",
            "        [-1.4387],\n",
            "        [-0.9587],\n",
            "        [-1.6782],\n",
            "        [-1.0496],\n",
            "        [-1.5638],\n",
            "        [-0.9883],\n",
            "        [-0.6585],\n",
            "        [-1.6045],\n",
            "        [-1.9270],\n",
            "        [-1.2465],\n",
            "        [-1.0694],\n",
            "        [-0.6117],\n",
            "        [-1.6931],\n",
            "        [-1.0491],\n",
            "        [-0.8088],\n",
            "        [-1.0627],\n",
            "        [-0.1437],\n",
            "        [-1.0499],\n",
            "        [-1.3966],\n",
            "        [-1.6390],\n",
            "        [-0.7477],\n",
            "        [-1.3773],\n",
            "        [-0.6723]], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "torch.Size([99, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 1e-1\n",
        "iterations = 10000\n",
        "for i in range(iterations):\n",
        "  preds = hypothesis(x,weights,bias)\n",
        "  cost = cost_calculator(preds,y)\n",
        "  if i%1000 == 0:\n",
        "    print(f\"Cost at iteration {i} is {cost}\")\n",
        "  cost.backward()\n",
        "  with pt.no_grad():\n",
        "    weights -= alpha * weights.grad\n",
        "    bias -= alpha * bias.grad\n",
        "    weights.grad.zero_()\n",
        "    bias.grad.zero_()\n",
        "\n",
        "percentage = precent_calc(initial_cost,cost)\n",
        "\n",
        "print(f\"\\nFinal cost: {cost}\")\n",
        "print(f\"\\nComparing Inital and Final Costs:\\n     Initial Cost = {initial_cost}\\n     Final Cost   = {cost}\\nThere has been {percentage:.2f} % decrease in Cost! \")\n",
        "\n",
        "\n",
        "print(f\"\\nFinal weights: \\n{weights}\")\n",
        "print(f\"\\nFinal bias: \\n{bias}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8z5Ips30Ln84",
        "outputId": "96dc34cf-438b-4f6f-9921-7ac0e8c10103"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost at iteration 0 is 0.9773365967640137\n",
            "Cost at iteration 1000 is 0.005751276667835142\n",
            "Cost at iteration 2000 is 0.005570002635471323\n",
            "Cost at iteration 3000 is 0.005567104177065947\n",
            "Cost at iteration 4000 is 0.005567034683451794\n",
            "Cost at iteration 5000 is 0.005567032862219627\n",
            "Cost at iteration 6000 is 0.005567032812793144\n",
            "Cost at iteration 7000 is 0.005567032811427767\n",
            "Cost at iteration 8000 is 0.005567032811389703\n",
            "Cost at iteration 9000 is 0.005567032811388638\n",
            "\n",
            "Final cost: 0.005567032811388606\n",
            "\n",
            "Comparing Inital and Final Costs:\n",
            "     Initial Cost = 0.9773365967640137\n",
            "     Final Cost   = 0.005567032811388606\n",
            "There has been 99.43 % decrease in Cost! \n",
            "\n",
            "Final weights: \n",
            "tensor([[ 0.0958],\n",
            "        [-0.0923],\n",
            "        [ 0.2639],\n",
            "        [-0.0704],\n",
            "        [ 0.0679]], dtype=torch.float64, requires_grad=True)\n",
            "\n",
            "Final bias: \n",
            "tensor([0.0350], dtype=torch.float64, requires_grad=True)\n"
          ]
        }
      ]
    }
  ]
}